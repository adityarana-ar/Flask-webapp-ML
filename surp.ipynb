{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2560)\n",
       "    (wpe): Embedding(2048, 2560)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\" # Can be replaced to test other models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval() # Setting to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to compute SURP Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprising_tokens(text, model, tokenizer, entropy_threshold = 2.0, prob_percentile=20):\n",
    "    # Tokenizing the input text\n",
    "    tokens = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "    input_ids = tokens['input_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits # Model's token prediction probabilities\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1) # Converting logits to probabilities\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1) # Computing entropy of the token predictions\n",
    "\n",
    "    gt_probs = probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1) # Getting the probabilities of the ground truth tokens\n",
    "\n",
    "    # Convert to numpy\n",
    "    entropy_values = entropy.squeeze().numpy()\n",
    "    gt_probs_values = gt_probs.squeeze().numpy()\n",
    "\n",
    "    # Identify surprising tokens (low entropy and low probability)\n",
    "    low_entropy_indices = np.where(entropy_values < entropy_threshold)[0]\n",
    "    prob_threshold = np.percentile(gt_probs_values, prob_percentile)\n",
    "    low_prob_indices = np.where(gt_probs_values < prob_threshold)[0]\n",
    "\n",
    "    suprising_indices = set(low_entropy_indices) & set(low_prob_indices)\n",
    "    surprising_tokens = [tokenizer.decode(input_ids[0, i].item()) for i in suprising_indices]\n",
    "\n",
    "    return surprising_tokens, gt_probs_values, entropy_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying text accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, model, tokenizer, lambda_threshold=-5.0):\n",
    "    \"\"\"\n",
    "    Classifies text as AI-generated or Human-Written based on SURP score.\n",
    "    \"\"\"\n",
    "    surprising_tokens, gt_probs, entropy = compute_surprising_tokens(text, model, tokenizer)\n",
    "\n",
    "    # Compute SURP Score (average log probability of surprising tokens)\n",
    "    surprising_indices = [i for i in range(len(gt_probs)) if tokenizer.decode([tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0][i]]) in surprising_tokens]\n",
    "\n",
    "    if surprising_indices:\n",
    "        surp_score = np.mean(np.log([gt_probs[idx] + 1e-9 for idx in surprising_indices]))\n",
    "    else:\n",
    "        surp_score = -10  # Assign a very low value if no surprising tokens are found\n",
    "\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"SURP Score: {surp_score}\")\n",
    "    print(f\"Surprising Tokens: {surprising_tokens}\")\n",
    "\n",
    "    return \"Human-Written\" if surp_score >= lambda_threshold else \"AI-Generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting program to the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Pipeline hazards arise in pipelined CPU architectures and can cause delays in instruction execution.\n",
      "SURP Score: -16.26364517211914\n",
      "Surprising Tokens: [' pip', 'el']\n",
      "AI Text Classification:  AI-Generated\n",
      "\n",
      "Text: Can’t believe Shannon’s entropy is helping me read a ML scientific paper\n",
      "SURP Score: -17.70323371887207\n",
      "Surprising Tokens: ['�', '�', '�', '�']\n",
      "Human Text Classification:  AI-Generated\n"
     ]
    }
   ],
   "source": [
    "text_ai = \"Pipeline hazards arise in pipelined CPU architectures and can cause delays in instruction execution.\"\n",
    "text_human = \"Can’t believe Shannon’s entropy is helping me read a ML scientific paper\"\n",
    "\n",
    "print(\"AI Text Classification: \", classify_text(text_ai, model, tokenizer))\n",
    "print(\"Human Text Classification: \", classify_text(text_human, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
